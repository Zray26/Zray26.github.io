1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-552819f9be8444f4.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-552819f9be8444f4.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-552819f9be8444f4.js"],"default"]
8:I[9665,[],"MetadataBoundary"]
a:I[9665,[],"OutletBoundary"]
d:I[4911,[],"AsyncMetadataOutlet"]
f:I[9665,[],"ViewportBoundary"]
11:I[6614,[],""]
:HL["/_next/static/css/92b53c90e215dddf.css","style"]
0:{"P":null,"b":"A9vueh0acPq0MzuWxouJT","p":"","c":["","publications",""],"i":false,"f":[[["",{"children":[["slug","publications","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/92b53c90e215dddf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Lei Zhou","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"December 16, 2025"}]]}]}]]}]]}],{"children":[["slug","publications","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L7",["$","$L8",null,{"children":"$L9"}],null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","u-jbL8Flv8JovCIQzNIpu",{"children":[["$","$Lf",null,{"children":"$L10"}],null]}],null]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:"$Sreact.suspense"
13:I[4911,[],"AsyncMetadata"]
15:I[6669,["17","static/chunks/17-9c04c9413a9f9f1f.js","357","static/chunks/357-37f1f8c73ae05597.js","748","static/chunks/748-92a57b0cb851fe9e.js","182","static/chunks/app/%5Bslug%5D/page-84615c228d27d4e0.js"],"default"]
9:["$","$12",null,{"fallback":null,"children":["$","$L13",null,{"promise":"$@14"}]}]
7:["$","div",null,{"className":"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12","children":[["$","$L15",null,{"config":{"type":"publication","title":"Publications","description":"A collection of my research work.","source":"publications.bib"},"publications":[{"id":"mimo2025embodied","title":"MiMo-Embodied: X-Embodied Foundation Model Technical Report","authors":[{"name":"Xiaomi Embodied Intelligence Team","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:0:tags","researchArea":"machine-learning","journal":"arXiv preprint arXiv:2511.16518","conference":"","url":"https://arxiv.org/abs/2511.16518","code":"https://github.com/XiaomiMiMo/MiMo-Embodied","abstract":"MiMo-Embodied is the first open-source foundation model that unifies embodied AI and autonomous driving, achieving state-of-the-art results on 17 embodied AI benchmarks and 12 autonomous driving benchmarks across perception, planning, spatial reasoning, and action understanding.","description":"The first open-source foundation model unifying embodied AI and autonomous driving.","selected":true,"preview":"MiMo-Embodied.png","bibtex":"@article{mimo2025embodied,\n  title = {MiMo-Embodied: X-Embodied Foundation Model Technical Report},\n  author = {{Xiaomi Embodied Intelligence Team}},\n  year = {2025},\n  journal = {arXiv preprint arXiv:2511.16518},\n  url = {https://arxiv.org/abs/2511.16518},\n  abstract = {MiMo-Embodied is the first open-source foundation model that unifies embodied AI and autonomous driving, achieving state-of-the-art results on 17 embodied AI benchmarks and 12 autonomous driving benchmarks across perception, planning, spatial reasoning, and action understanding.}\n}"},{"id":"li2025vitra","title":"Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos","authors":[{"name":"Qixiu Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yu Deng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yaobo Liang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Luo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lei Zhou","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Chengtang Yao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lingqi Zeng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiyuan Feng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Huizhi Liang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sicheng Xu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yizhong Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lily Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Dong Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiaolong Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Baining Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:1:tags","researchArea":"machine-learning","journal":"Under review","conference":"","url":"https://arxiv.org/abs/2510.21571","website":"https://microsoft.github.io/VITRA/","code":"https://github.com/microsoft/VITRA/","abstract":"VITRA pretrains Vision-Language-Action models for manipulation using over 1M episodes of unlabeled, real-world human hand videos, treating hands as dexterous robot end-effectors. By converting egocentric human activity into robot-compatible VLA data and training a causal action transformer, VITRA achieves strong zero-shot action prediction.","description":"Pretraining VLA models using 1M+ episodes of unlabeled human hand videos.","selected":false,"preview":"vitra.jpg","bibtex":"@article{li2025vitra,\n  title = {Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos},\n  author = {Qixiu Li and Yu Deng and Yaobo Liang and Lin Luo and Lei Zhou and Chengtang Yao and Lingqi Zeng and Zhiyuan Feng and Huizhi Liang and Sicheng Xu and Yizhong Zhang and Xi Chen and Hao Chen and Lily Sun and Dong Chen and Jiaolong Yang and Baining Guo},\n  year = {2025},\n  journal = {Under review},\n  url = {https://arxiv.org/abs/2510.21571},\n  website = {https://microsoft.github.io/VITRA/},\n  abstract = {VITRA pretrains Vision-Language-Action models for manipulation using over 1M episodes of unlabeled, real-world human hand videos, treating hands as dexterous robot end-effectors. By converting egocentric human activity into robot-compatible VLA data and training a causal action transformer, VITRA achieves strong zero-shot action prediction.}\n}"},{"id":"zhang2025falcon","title":"From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors","authors":[{"name":"Zhengshen Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yalun Dai","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhengbang Zhu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lei Zhou","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Chenchen Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Dong Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Francis E. H. Tay","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sijin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ziwei Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yuxiao Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xinghang Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Pan Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:2:tags","researchArea":"machine-learning","journal":"Under review","conference":"","url":"https://arxiv.org/abs/2510.17439","website":"https://falcon-vla.github.io/","abstract":"FALCON is a vision-language-action model that improves robot manipulation by injecting rich 3D spatial tokens into the action head, giving robots stronger geometric understanding from RGB or optional depth inputs. It achieves state-of-the-art performance on simulation and real-world benchmarks.","description":"Injecting 3D spatial tokens into VLA models for better geometric understanding.","selected":false,"preview":"falcon_pipeline.png","bibtex":"@article{zhang2025falcon,\n  title = {From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors},\n  author = {Zhengshen Zhang and Hao Li and Yalun Dai and Zhengbang Zhu and Lei Zhou and Chenchen Liu and Dong Wang and Francis E. H. Tay and Sijin Chen and Ziwei Liu and Yuxiao Liu and Xinghang Li and Pan Zhou},\n  year = {2025},\n  journal = {Under review},\n  url = {https://arxiv.org/abs/2510.17439},\n  website = {https://falcon-vla.github.io/},\n  abstract = {FALCON is a vision-language-action model that improves robot manipulation by injecting rich 3D spatial tokens into the action head, giving robots stronger geometric understanding from RGB or optional depth inputs. It achieves state-of-the-art performance on simulation and real-world benchmarks.}\n}"},{"id":"wang2025unigrasp","title":"UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping","authors":[{"name":"Wenbo Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Fangyun Wei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lei Zhou","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Xi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Luo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaohan Yi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yizhong Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yaobo Liang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chang Xu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Lu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiaolong Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Baining Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:3:tags","researchArea":"transformer-architectures","journal":"","conference":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","url":"https://arxiv.org/abs/2412.02699","website":"https://dexhand.github.io/UniGraspTransformer/","abstract":"UniGraspTransformer is a Transformer-based network for dexterous robotic grasping that streamlines training by distilling grasp trajectories from individually trained policy networks into a single universal model. It scales effectively with up to 12 self-attention blocks and generalizes well to diverse objects.","description":"A Transformer-based network for dexterous robotic grasping using policy distillation.","selected":true,"preview":"grasp_real.mp4","bibtex":"@inproceedings{wang2025unigrasp,\n  title = {UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping},\n  author = {Wenbo Wang and Fangyun Wei and Lei Zhou and Xi Chen and Lin Luo and Xiaohan Yi and Yizhong Zhang and Yaobo Liang and Chang Xu and Yan Lu and Jiaolong Yang and Baining Guo},\n  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2025},\n  url = {https://arxiv.org/abs/2412.02699},\n  website = {https://dexhand.github.io/UniGraspTransformer/},\n  abstract = {UniGraspTransformer is a Transformer-based network for dexterous robotic grasping that streamlines training by distilling grasp trajectories from individually trained policy networks into a single universal model. It scales effectively with up to 12 self-attention blocks and generalizes well to diverse objects.}\n}"},{"id":"zhang2024dexgrasp","title":"DexGrasp-Diffusion: Diffusion-based Unified Functional Grasp Synthesis Pipeline for Multi-Dexterous Robotic Hands","authors":[{"name":"Zhengshen Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lei Zhou","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Chenchen Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiyang Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sheng Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruiteng Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Marcelo H. Ang Jr.","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Francis EH Tay","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:4:tags","researchArea":"machine-learning","journal":"","conference":"International Symposium on Robotics Research (ISRR)","url":"https://arxiv.org/abs/2407.09899","website":"https://zray26.github.io/dexgrasp/","abstract":"We present an innovative end-to-end pipeline for synthesizing functional grasps for diverse dexterous robotic hands, integrating a diffusion model for grasp estimation with a discriminator for validating grasps based on object affordances.","description":"End-to-end pipeline for functional grasp synthesis using diffusion models.","selected":false,"preview":"Denoising.mp4","bibtex":"@inproceedings{zhang2024dexgrasp,\n  title = {DexGrasp-Diffusion: Diffusion-based Unified Functional Grasp Synthesis Pipeline for Multi-Dexterous Robotic Hands},\n  author = {Zhengshen Zhang and Lei Zhou and Chenchen Liu and Zhiyang Liu and Sheng Guo and Ruiteng Zhao and Marcelo H. Ang Jr. and Francis EH Tay},\n  booktitle = {International Symposium on Robotics Research (ISRR)},\n  year = {2024},\n  url = {https://arxiv.org/abs/2407.09899},\n  website = {https://zray26.github.io/dexgrasp/},\n  abstract = {We present an innovative end-to-end pipeline for synthesizing functional grasps for diverse dexterous robotic hands, integrating a diffusion model for grasp estimation with a discriminator for validating grasps based on object affordances.}\n}"},{"id":"zhou2024yoso","title":"You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects","authors":[{"name":"Lei Zhou","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Haozhe Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhengshen Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiyang Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Francis EH Tay","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Marcelo H. Ang Jr.","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:5:tags","researchArea":"machine-learning","journal":"","conference":"IEEE International Conference on Robotics and Automation (ICRA)","url":"https://arxiv.org/abs/2404.03462","website":"https://zray26.github.io/yoso/","abstract":"Dynamically reconstructing scene point cloud by transforming NeRF generated object meshes back into workspace with tracked object pose. Scene Reconstruction module runs at 9.2 FPS and whole pipeline (including grasp estimation) runs at 2.8 FPS.","description":"Dynamic scene reconstruction using NeRF for 6-DoF robotic grasping.","selected":true,"preview":"full_pc.mp4","bibtex":"@inproceedings{zhou2024yoso,\n  title = {You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects},\n  author = {Lei Zhou and Haozhe Wang and Zhengshen Zhang and Zhiyang Liu and Francis EH Tay and Marcelo H. Ang Jr.},\n  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},\n  year = {2024},\n  url = {https://arxiv.org/abs/2404.03462},\n  website = {https://zray26.github.io/yoso/},\n  abstract = {Dynamically reconstructing scene point cloud by transforming NeRF generated object meshes back into workspace with tracked object pose. Scene Reconstruction module runs at 9.2 FPS and whole pipeline (including grasp estimation) runs at 2.8 FPS.}\n}"},{"id":"zhou2023drpose","title":"DR-Pose: A Two-stage Deformation-and-Registration Pipeline for Category-level 6D Object Pose Estimation","authors":[{"name":"Lei Zhou","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiyang Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Runze Gan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haozhe Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Marcelo H. Ang Jr","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:6:tags","researchArea":"machine-learning","journal":"","conference":"IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","url":"https://arxiv.org/abs/2309.01925","website":"https://zray26.github.io/drpose/","code":"https://github.com/Zray26/DR-Pose","abstract":"DR-Pose introduces a two-stage pipeline enhancing category-level 6D object pose estimation by first completing unseen parts of objects to guide shape prior deformation, followed by scaled registration for precise pose prediction.","description":"Enhancing category-level 6D object pose estimation via deformation and registration.","selected":false,"preview":"video_scene_3.mp4","bibtex":"@inproceedings{zhou2023drpose,\n  title = {DR-Pose: A Two-stage Deformation-and-Registration Pipeline for Category-level 6D Object Pose Estimation},\n  author = {Lei Zhou and Zhiyang Liu and Runze Gan and Haozhe Wang and Marcelo H. Ang Jr},\n  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n  year = {2023},\n  url = {https://arxiv.org/abs/2309.01925},\n  website = {https://zray26.github.io/drpose/},\n  abstract = {DR-Pose introduces a two-stage pipeline enhancing category-level 6D object pose estimation by first completing unseen parts of objects to guide shape prior deformation, followed by scaled registration for precise pose prediction.}\n}"}]}],false,false]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:null
14:{"metadata":[["$","title","0",{"children":"Publications | Lei Zhou"}],["$","meta","1",{"name":"description","content":"A collection of my research work."}],["$","meta","2",{"name":"author","content":"Lei Zhou"}],["$","meta","3",{"name":"keywords","content":"Lei Zhou,PhD,Research,National University of Singapore"}],["$","meta","4",{"name":"creator","content":"Lei Zhou"}],["$","meta","5",{"name":"publisher","content":"Lei Zhou"}],["$","meta","6",{"property":"og:title","content":"Lei Zhou"}],["$","meta","7",{"property":"og:description","content":"PhD student at the National University of Singapore."}],["$","meta","8",{"property":"og:site_name","content":"Lei Zhou's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Lei Zhou"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at the National University of Singapore."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
e:{"metadata":"$14:metadata","error":null,"digest":"$undefined"}
