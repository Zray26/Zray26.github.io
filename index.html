<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lei Zhou</title>

    <meta name="author" content="Lei Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Lei Zhou
                <p>
                I am a Ph.D. student at the Advanced Robotics Centre, National University of Singapore, advised by <strong>Professor Marcelo H. Ang, Jr.</strong>. I received my B.E. in Mechanical Engineering from Huazhong University of Science and Technology. I previously worked as a research intern at <strong>Microsoft Research Asia</strong>, where I explored embodied AI topics under the mentorship of <strong>Dr. Jiaolong Yang</strong>. Since June 2025, I have been a research intern at <strong>Xiaomi Technology</strong> in the <strong>Xiaomi Embodied Intelligence Team</strong>, working under <strong>Dr. Long Chen</strong> on multimodal large language models, vision–language–action systems, and world models.
                </p>
                <p style="text-align:center">
                  <a href="mailto:leizhou@u.nus.edu">Email</a> &nbsp;/&nbsp;
                <a href="data/resume_github.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp; -->
				  <!-- <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp;
				  <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/zray26/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <!-- <a href="images/leizhou.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/leizhou.jpg" class="hoverZoomLink"></a> -->
                <a href="images/leizhou_crop.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/leizhou_crop.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
              <p>
                I work on 3D human sensing, multimodal large language models (MLLMs), vision–language–action (VLA) systems, and world models aimed at improving robotic manipulation capabilities.
              </p>
              </td>
            </tr>          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


      <!-- Icon row -->
      <div style="display:flex;gap:20px;align-items:center;margin-top:10px;flex-wrap:wrap;">
        
        <img src="images/nus_logo.jpg" alt="National University of Singapore" style="height:80px;">
        
        <img src="images/msra_logo.png" alt="Microsoft Research Asia" style="height:80px;">
        
        <img src="images/xiaomi_logo.png" alt="Xiaomi Technology" style="height:80px;">

      </div>


      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id="smerf_image">
              <img src="images/MiMo-Embodied.png" alt="MiMo-Embodied" style="width:100%;max-width:100%;object-fit:cover;">
            </div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://github.com/XiaomiMiMo/MiMo-Embodied">
            <span class="papertitle">MiMo-Embodied: X-Embodied Foundation Model Technical Report</span>
          </a>
          <br>
          Xiaomi Embodied Intelligence Team
          <br>
          <em>arXiv</em>
          <br>
          <a href="https://arxiv.org/abs/2511.16518">Technical Report</a>
          /
          <a href="https://huggingface.co/XiaomiMiMo/MiMo-Embodied-7B">model</a>
          /
          <a href="https://github.com/XiaomiMiMo/MiMo-Embodied">code</a>
          <p></p>
          <p>
            MiMo-Embodied is the first open-source foundation model that unifies embodied AI and autonomous driving, achieving state-of-the-art results on 17 embodied AI benchmarks and 12 autonomous driving benchmarks across perception, planning, spatial reasoning, and action understanding. It demonstrates strong cross-domain transfer and significantly outperforms both general multimodal models and specialized driving/robotics systems.
          </p>      </td>
      </tr>



      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id="smerf_image">
              <img src="images/vitra.jpg" alt="Vitra Teaser Pipeline" style="width:100%;max-width:100%;object-fit:cover;">
            </div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://microsoft.github.io/VITRA/">
            <span class="papertitle">Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos</span>
          </a>
          <br>
          Qixiu Li, 
          Yu Deng, 
          Yaobo Liang, 
          Lin Luo, 
          <strong>Lei Zhou</strong>,
          Chengtang Yao, 
          Lingqi Zeng, 
          Zhiyuan Feng, 
          Huizhi Liang, 
          Sicheng Xu, 
          Yizhong Zhang, 
          Xi Chen, 
          Hao Chen, 
          Lily Sun, 
          Dong Chen, 
          Jiaolong Yang, 
          Baining Guo
          <br>
          <em>Under review</em>, 2025
          <br>
          <a href="https://microsoft.github.io/VITRA/">project page</a>
          /
          <a href="https://arxiv.org/abs/2510.21571">paper</a>
          /
          <a href="https://github.com/microsoft/VITRA/">code</a>
          /
          <p></p>
          <p>
            VITRA pretrains Vision-Language-Action models for manipulation using over 1M episodes of unlabeled, real-world human hand videos, treating hands as dexterous robot end-effectors. By converting egocentric human activity into robot-compatible VLA data and training a causal action transformer, VITRA achieves strong zero-shot action prediction and enables effective few-shot adaptation to real robotic tasks.
          </p>      </td>
      </tr>



      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id="smerf_image">
              <img src="images/falcon_pipeline.png" alt="Falcon Pipeline" style="width:100%;max-width:100%;object-fit:cover;">
            </div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://falcon-vla.github.io/">
            <span class="papertitle">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</span>
          </a>
          <br>
            Zhengshen Zhang, 
            Hao Li, 
            Yalun Dai, 
            Zhengbang Zhu, 
          <strong>Lei Zhou</strong>,
            Chenchen Liu, 
            Dong Wang, 
            Francis E. H. Tay,
            Sijin Chen, 
            Ziwei Liu, 
            Yuxiao Liu, 
            Xinghang Li, 
            Pan Zhou
          <br>
          <em>Under review</em>, 2025
          <br>
          <a href="https://falcon-vla.github.io/">project page</a>
          <p></p>
          <p>
            FALCON is a vision-language-action model that improves robot manipulation by injecting rich 3D spatial tokens into the action head, giving robots stronger geometric understanding from RGB or optional depth inputs. It achieves state-of-the-art performance on simulation and real-world benchmarks, showing superior generalization, spatial reasoning, and few-shot adaptability.
          </p>      </td>
      </tr>


      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
            <source src="images/grasp_real.mp4" type="video/mp4">
            Your browser does not support the video tag.
          <!-- </video></div> -->
            </video>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dexhand.github.io/UniGraspTransformer/">
            <span class="papertitle">UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping </span>
          </a>
          <br>
          Wenbo Wang,
          Fangyun Wei,
          <strong>Lei Zhou</strong>,
          Xi Chen,
          Lin Luo,
          Xiaohan Yi,
          Yizhong Zhang,
          Yaobo Liang,
          Chang Xu,
          Yan Lu,
          Jiaolong Yang,
          Baining, Guo
          <br>
          <em>CVPR</em>, 2025
          <br>
          <a href="https://dexhand.github.io/UniGraspTransformer/">project page</a>
          <p></p>
          <p>
            UniGraspTransformer is a Transformer-based network for dexterous robotic grasping that streamlines training by distilling grasp trajectories from individually trained policy networks into a single universal model. It scales effectively with up to 12 self-attention blocks, generalizes well to diverse objects and real-world settings, and outperforms UniDexGrasp++ with higher success rates across seen and unseen objects.
          </p>      </td>
      </tr>
        

    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/Denoising.mp4" type="video/mp4">
          Your browser does not support the video tag.
        <!-- </video></div> -->
          </video>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://zray26.github.io/dexgrasp/">
          <span class="papertitle">DexGrasp-Diffusion: Diffusion-based Unified Functional Grasp Synthesis Pipeline for Multi-Dexterous Robotic Hands </span>
        </a>
        <br>
        Zhengshen Zhang,
        <strong>Lei Zhou</strong>,
    Chenchen Liu,
    Zhiyang Liu,
    Sheng Guo,
    Ruiteng Zhao,
    Marcelo H. Ang Jr.,
    Francis EH Tay
        <br>
        <em>ISRR</em>, 2024
        <br>
        <a href="https://zray26.github.io/dexgrasp/">project page</a>
        <p></p>
        <p>
        We presents an innovative end-to-end pipeline for synthesizing functional grasps for diverse dexterous robotic hands, integrating a diffusion model for grasp estimation with a discriminator for validating grasps based on object affordances.
        </p>      </td>
    </tr>

    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/full_pc.mp4" type="video/mp4">
          Your browser does not support the video tag.
        <!-- </video></div> -->
          </video>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://zray26.github.io/yoso/">
          <span class="papertitle">You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects</span>
        </a>
        <br>
		<strong>Lei Zhou</strong>,
		Haozhe Wang,
		Zhengshen Zhang,
		Zhiyang Liu,
		Francis EH Tay,
		Marcelo H. Ang Jr.
        <br>
        <em>ICRA</em>, 2024
        <br>
        <a href="https://zray26.github.io/yoso/">project page</a>
        /
        <a href="https://arxiv.org/abs/2404.03462">arXiv</a>
        <p></p>
        <p>
        Dynamically reconstructing scene point cloud by transforming NeRF generated object meshes back into workspace with tracked object pose. Scene Reconstruction module runs at 9.2 FPS and whole pipeline (including grasp estimation) runs at 2.8 FPS.
        </p>      </td>
    </tr>
	


    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='dr'><video  width=100% muted autoplay loop>
          <source src="images/video_scene_3.mp4" type="video/mp4">

        <!-- </video></div> -->
          </video>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://zray26.github.io/drpose/">
          <span class="papertitle">DR-Pose: A Two-stage Deformation-and-Registration Pipeline for Category-level 6D Object Pose Estimation</span>
        </a>
        <br>
        <strong>Lei Zhou</strong>,
        Zhiyang Liu,
        Runze Gan,
        Haozhe Wang,
        Marcelo H. Ang Jr
        <br>
          <em>IROS</em>, 2023
        <br>
        <a href="https://zray26.github.io/drpose/">project page</a>
        /
        <a href="https://arxiv.org/abs/2309.01925">arXiv</a>
        /
        <a href="https://github.com/Zray26/DR-Pose">code</a>
        <p></p>
        <p>
        
DR-Pose introduces a two-stage pipeline enhancing category-level 6D object pose estimation by first completing unseen parts of objects to guide shape prior deformation, followed by scaled registration for precise pose prediction. This method significantly improves pose estimation accuracy over existing techniques, as demonstrated on benchmark datasets.
        </p>
      </td>
 
    </table>

    <footer class="footer">
  
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This webpage template was inspired from <a
                  href="https://jonbarron.info/">jonbarron</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>


  </body>
</html>
