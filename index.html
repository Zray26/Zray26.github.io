<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lei Zhou</title>

    <meta name="author" content="Lei Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Lei Zhou
                </p>
                <p>I'm a Ph.D. student in the Advanced Robotics Centre at National University of Singapore, advised by Professor Marcelo H. Ang. Jr.. I received my B.E. in Mechanical Engineering from Huazhong University of Science and Technology.
                </p>
                <p style="text-align:center">
                  <a href="mailto:leizhou@u.nus.edu">Email</a> &nbsp;/&nbsp;
                <a href="data/ZL_cv.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp; -->
				  <!-- <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp;
				  <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/zray26/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <!-- <a href="images/leizhou.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/leizhou.jpg" class="hoverZoomLink"></a> -->
                <a href="images/leizhou_crop.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/leizhou_crop.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  I'm interested in computer vision, generative AI, and robotic manipulation. Most of my previous research is about perception for robotic manipulation, including object pose estimation, robotic grasp generation, visual affordance detection and scene reconstruction. Recently I am exploring topics such as generative AI for human/robot-scene interaction and 3D content generation. 
                </p>
              </td>
            </tr>          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
            <source src="images/grasp_real.mp4" type="video/mp4">
            Your browser does not support the video tag.
          <!-- </video></div> -->
            </video>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://zray26.github.io/dexgrasp/">
            <span class="papertitle">UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping </span>
          </a>
          <br>
          Wenbo Wang,
          Fangyun Wei,
          <strong>Lei Zhou</strong>,
          <br>
          <em>ISRR</em>, 2024
          <br>
          <a href="https://dexhand.github.io/UniGraspTransformer/">project page</a>
          <p></p>
          <p>
            UniGraspTransformer is a Transformer-based network for dexterous robotic grasping that streamlines training by distilling grasp trajectories from individually trained policy networks into a single universal model. It scales effectively with up to 12 self-attention blocks, generalizes well to diverse objects and real-world settings, and outperforms UniDexGrasp++ with higher success rates across seen and unseen objects.
          </p>      </td>
      </tr>
        

    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/Denoising.mp4" type="video/mp4">
          Your browser does not support the video tag.
        <!-- </video></div> -->
          </video>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://zray26.github.io/dexgrasp/">
          <span class="papertitle">DexGrasp-Diffusion: Diffusion-based Unified Functional Grasp Synthesis Pipeline for Multi-Dexterous Robotic Hands </span>
        </a>
        <br>
        Zhengshen Zhang,
        <strong>Lei Zhou</strong>,
    Chenchen Liu,
    Zhiyang Liu,
    Sheng Guo,
    Ruiteng Zhao,
    Marcelo H. Ang Jr.,
    Francis EH Tay
        <br>
        <em>ISRR</em>, 2024
        <br>
        <a href="https://zray26.github.io/dexgrasp/">project page</a>
        <p></p>
        <p>
        We presents an innovative end-to-end pipeline for synthesizing functional grasps for diverse dexterous robotic hands, integrating a diffusion model for grasp estimation with a discriminator for validating grasps based on object affordances.
        </p>      </td>
    </tr>

    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/full_pc.mp4" type="video/mp4">
          Your browser does not support the video tag.
        <!-- </video></div> -->
          </video>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://zray26.github.io/yoso/">
          <span class="papertitle">You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects</span>
        </a>
        <br>
		<strong>Lei Zhou</strong>,
		Haozhe Wang,
		Zhengshen Zhang,
		Zhiyang Liu,
		Francis EH Tay,
		Marcelo H. Ang Jr.
        <br>
        <em>ICRA</em>, 2024
        <br>
        <a href="https://zray26.github.io/yoso/">project page</a>
        /
        <a href="https://arxiv.org/abs/2404.03462">arXiv</a>
        <p></p>
        <p>
        Dynamically reconstructing scene point cloud by transforming NeRF generated object meshes back into workspace with tracked object pose. Scene Reconstruction module runs at 9.2 FPS and whole pipeline (including grasp estimation) runs at 2.8 FPS.
        </p>      </td>
    </tr>
	


    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='dr'><video  width=100% muted autoplay loop>
          <source src="images/video_scene_3.mp4" type="video/mp4">

        <!-- </video></div> -->
          </video>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://zray26.github.io/drpose/">
          <span class="papertitle">DR-Pose: A Two-stage Deformation-and-Registration Pipeline for Category-level 6D Object Pose Estimation</span>
        </a>
        <br>
        <strong>Lei Zhou</strong>,
        Zhiyang Liu,
        Runze Gan,
        Haozhe Wang,
        Marcelo H. Ang Jr
        <br>
          <em>IROS</em>, 2023
        <br>
        <a href="https://zray26.github.io/drpose/">project page</a>
        /
        <a href="https://arxiv.org/abs/2309.01925">arXiv</a>
        /
        <a href="https://github.com/Zray26/DR-Pose">code</a>
        <p></p>
        <p>
        
DR-Pose introduces a two-stage pipeline enhancing category-level 6D object pose estimation by first completing unseen parts of objects to guide shape prior deformation, followed by scaled registration for precise pose prediction. This method significantly improves pose estimation accuracy over existing techniques, as demonstrated on benchmark datasets.
        </p>
      </td>
 
    </table>

    <footer class="footer">
  
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This webpage template was inspired from <a
                  href="https://jonbarron.info/">jonbarron</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>


  </body>
</html>
