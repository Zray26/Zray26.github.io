1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-552819f9be8444f4.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-552819f9be8444f4.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-552819f9be8444f4.js"],"default"]
7:I[7437,["17","static/chunks/17-9c04c9413a9f9f1f.js","357","static/chunks/357-37f1f8c73ae05597.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-92a57b0cb851fe9e.js","974","static/chunks/app/page-b5b69f608318cc4c.js"],"default"]
8:I[9507,["17","static/chunks/17-9c04c9413a9f9f1f.js","357","static/chunks/357-37f1f8c73ae05597.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-92a57b0cb851fe9e.js","974","static/chunks/app/page-b5b69f608318cc4c.js"],"default"]
a:I[5218,["17","static/chunks/17-9c04c9413a9f9f1f.js","357","static/chunks/357-37f1f8c73ae05597.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-92a57b0cb851fe9e.js","974","static/chunks/app/page-b5b69f608318cc4c.js"],"default"]
b:I[9665,[],"MetadataBoundary"]
d:I[9665,[],"OutletBoundary"]
10:I[4911,[],"AsyncMetadataOutlet"]
12:I[9665,[],"ViewportBoundary"]
14:I[6614,[],""]
:HL["/_next/static/css/92b53c90e215dddf.css","style"]
9:T65d,I am a Ph.D. student at the [National University of Singapore (NUS)](https://nus.edu.sg), advised by **Prof. Marcelo H. Ang Jr.** I received my B.E. in Mechanical Engineering from **Huazhong University of Science and Technology (HUST)**.

My research lies at the intersection of **Embodied AI**, **Computer Vision**, and **Robotics**. Specifically, I am interested in leveraging large-scale human video data to train generalist robot policies, developing Vision-Language-Action (VLA) models, and exploring World Models for robotic manipulation.

Currently, I am a Research Intern at **Xiaomi Technology** (Embodied Intelligence Team), working closely with **Dr. Long Chen**. Prior to this, I was a Research Intern at **Microsoft Research Asia (MSRA)**, where I worked closely with **Dr. Jiaolong Yang** and **Dr. Yu Deng**.


### News
*   **Nov 2025:** Released **MiMo-Embodied**, a foundation model achieving SOTA on 17 embodied AI benchmarks.
*   **May 2025:** Joined **Xiaomi Technology** (Embodied Intelligence Team) as a Research Intern.
*   **May 2025:** Awarded the **Microsoft “Stars of Tomorrow” Internship** honor from MSRA.
*   **Feb 2025:** One paper (**UniGraspTransformer**) accepted to **CVPR 2025**.
*   **Dec 2024:** One paper (**DexGrasp-Diffusion**) accepted to **ISRR 2024**.
*   **Jul 2024:** Two papers (on **Robotic Packing** and **3D Affordance Keypoints**) accepted to **IROS 2024**.
*   **May 2024:** Joined **Microsoft Research Asia (MSRA)** as a Research Intern.
*   **Jan 2024:** One paper (**YOSO**) accepted to **ICRA 2024**.
*   **Jul 2023:** One paper (**DR-Pose**) accepted to **IROS 2023**.0:{"P":null,"b":"A9vueh0acPq0MzuWxouJT","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/92b53c90e215dddf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Lei Zhou","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"December 16, 2025"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Lei Zhou","title":"PhD Student","institution":"National University of Singapore","avatar":"/images/leizhou_crop.jpg"},"social":{"email":"zhoulei0426@gmail.com","location":"Singapore, Singapore","location_url":"https://maps.google.com","location_details":["Advanced Robotics Centre","5 Engineering Dr 1, #07-01, Singapore 117608"],"google_scholar":"https://scholar.google.com/citations?user=VhToj4wAAAAJ&hl=en&oi=sra","github":"https://github.com/zray26/","linkedin":"https://www.linkedin.com/in/lei-zhou-nusarc"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["Robotic Manipulation","Vision-Language-Action Model","3D Human Sensing"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"$9","title":"About"}],["$","$La","featured_publications",{"publications":[{"id":"mimo2025embodied","title":"MiMo-Embodied: X-Embodied Foundation Model Technical Report","authors":[{"name":"Xiaomi Embodied Intelligence Team","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"machine-learning","journal":"arXiv preprint arXiv:2511.16518","conference":"","url":"https://arxiv.org/abs/2511.16518","code":"https://github.com/XiaomiMiMo/MiMo-Embodied","abstract":"MiMo-Embodied is the first open-source foundation model that unifies embodied AI and autonomous driving, achieving state-of-the-art results on 17 embodied AI benchmarks and 12 autonomous driving benchmarks across perception, planning, spatial reasoning, and action understanding.","description":"The first open-source foundation model unifying embodied AI and autonomous driving.","selected":true,"preview":"MiMo-Embodied.png","bibtex":"@article{mimo2025embodied,\n  title = {MiMo-Embodied: X-Embodied Foundation Model Technical Report},\n  author = {{Xiaomi Embodied Intelligence Team}},\n  year = {2025},\n  journal = {arXiv preprint arXiv:2511.16518},\n  url = {https://arxiv.org/abs/2511.16518},\n  abstract = {MiMo-Embodied is the first open-source foundation model that unifies embodied AI and autonomous driving, achieving state-of-the-art results on 17 embodied AI benchmarks and 12 autonomous driving benchmarks across perception, planning, spatial reasoning, and action understanding.}\n}"},{"id":"wang2025unigrasp","title":"UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping","authors":[{"name":"Wenbo Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Fangyun Wei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lei Zhou","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Xi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Luo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaohan Yi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yizhong Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yaobo Liang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chang Xu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Lu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiaolong Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Baining Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"transformer-architectures","journal":"","conference":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","url":"https://arxiv.org/abs/2412.02699","website":"https://dexhand.github.io/UniGraspTransformer/","abstract":"UniGraspTransformer is a Transformer-based network for dexterous robotic grasping that streamlines training by distilling grasp trajectories from individually trained policy networks into a single universal model. It scales effectively with up to 12 self-attention blocks and generalizes well to diverse objects.","description":"A Transformer-based network for dexterous robotic grasping using policy distillation.","selected":true,"preview":"grasp_real.mp4","bibtex":"@inproceedings{wang2025unigrasp,\n  title = {UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping},\n  author = {Wenbo Wang and Fangyun Wei and Lei Zhou and Xi Chen and Lin Luo and Xiaohan Yi and Yizhong Zhang and Yaobo Liang and Chang Xu and Yan Lu and Jiaolong Yang and Baining Guo},\n  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2025},\n  url = {https://arxiv.org/abs/2412.02699},\n  website = {https://dexhand.github.io/UniGraspTransformer/},\n  abstract = {UniGraspTransformer is a Transformer-based network for dexterous robotic grasping that streamlines training by distilling grasp trajectories from individually trained policy networks into a single universal model. It scales effectively with up to 12 self-attention blocks and generalizes well to diverse objects.}\n}"},{"id":"zhou2024yoso","title":"You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects","authors":[{"name":"Lei Zhou","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Haozhe Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhengshen Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiyang Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Francis EH Tay","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Marcelo H. Ang Jr.","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"IEEE International Conference on Robotics and Automation (ICRA)","url":"https://arxiv.org/abs/2404.03462","website":"https://zray26.github.io/yoso/","abstract":"Dynamically reconstructing scene point cloud by transforming NeRF generated object meshes back into workspace with tracked object pose. Scene Reconstruction module runs at 9.2 FPS and whole pipeline (including grasp estimation) runs at 2.8 FPS.","description":"Dynamic scene reconstruction using NeRF for 6-DoF robotic grasping.","selected":true,"preview":"full_pc.mp4","bibtex":"@inproceedings{zhou2024yoso,\n  title = {You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects},\n  author = {Lei Zhou and Haozhe Wang and Zhengshen Zhang and Zhiyang Liu and Francis EH Tay and Marcelo H. Ang Jr.},\n  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},\n  year = {2024},\n  url = {https://arxiv.org/abs/2404.03462},\n  website = {https://zray26.github.io/yoso/},\n  abstract = {Dynamically reconstructing scene point cloud by transforming NeRF generated object meshes back into workspace with tracked object pose. Scene Reconstruction module runs at 9.2 FPS and whole pipeline (including grasp estimation) runs at 2.8 FPS.}\n}"}],"title":"Selected Publications","enableOnePageMode":false}]],false,false,false]}]]}]]}]}],["$","$Lb",null,{"children":"$Lc"}],null,["$","$Ld",null,{"children":["$Le","$Lf",["$","$L10",null,{"promise":"$@11"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","DxjmsLwSneAdOFsmbdC3b",{"children":[["$","$L12",null,{"children":"$L13"}],null]}],null]}],false]],"m":"$undefined","G":["$14","$undefined"],"s":false,"S":true}
15:"$Sreact.suspense"
16:I[4911,[],"AsyncMetadata"]
c:["$","$15",null,{"fallback":null,"children":["$","$L16",null,{"promise":"$@17"}]}]
f:null
13:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
e:null
17:{"metadata":[["$","title","0",{"children":"Lei Zhou"}],["$","meta","1",{"name":"description","content":"PhD student at the National University of Singapore."}],["$","meta","2",{"name":"author","content":"Lei Zhou"}],["$","meta","3",{"name":"keywords","content":"Lei Zhou,PhD,Research,National University of Singapore"}],["$","meta","4",{"name":"creator","content":"Lei Zhou"}],["$","meta","5",{"name":"publisher","content":"Lei Zhou"}],["$","meta","6",{"property":"og:title","content":"Lei Zhou"}],["$","meta","7",{"property":"og:description","content":"PhD student at the National University of Singapore."}],["$","meta","8",{"property":"og:site_name","content":"Lei Zhou's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Lei Zhou"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at the National University of Singapore."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
11:{"metadata":"$17:metadata","error":null,"digest":"$undefined"}
